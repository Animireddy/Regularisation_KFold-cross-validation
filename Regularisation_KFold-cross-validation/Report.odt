
Roll Number: 20161191
Name: Animi Reddy


Lambda vs MSE graph for L1-Regularisation:


Lambda vs MSE graph for L2-Regularisation:


output of L1-Regularisation for various lambda:

MSE = 0.0036735711942558684
MAE = 0.032985698442968045
MPE = -0.041640869187917706
Lambda = 0.1
MSE = 0.0036746090239832756
MAE = 0.032938269597210176
MPE = -0.04171059631509883
Lambda = 0.2
MSE = 0.003675858320902848
MAE = 0.03289440290076558
MPE = -0.04178032344227943
Lambda = 0.30000000000000004
MSE = 0.0036773190850147353
MAE = 0.03285552046728948
MPE = -0.0418500505694622
Lambda = 0.4
MSE = 0.0036789913163186743
MAE = 0.032817311773889364
MPE = -0.04191977769664237
Lambda = 0.5
MSE = 0.003680875014814886
MAE = 0.03277910308048952
MPE = -0.04198950482382396
Lambda = 0.6000000000000001
MSE = 0.00368297018050323
MAE = 0.03274089438708947
MPE = -0.04205923195100438
Lambda = 0.7000000000000001
MSE = 0.003685276813383865
MAE = 0.03270268569368977
MPE = -0.04212895907818662
Lambda = 0.8
MSE = 0.0036877949134566127
MAE = 0.03266447700028974
MPE = -0.042198686205367306
Lambda = 0.9
MSE = 0.0036905244807216193
MAE = 0.03262626830688996
MPE = -0.042268413332549236
Lambda = 1.0
MSE = 0.0036934655151788184
MAE = 0.032588059613490185
MPE = -0.04233814045973099

output of L2-Regularisation for various lambda:

Lambda = 0.0
MSE = 0.0018928827460640318
MAE = 0.026077676198986214
MPE = 0.00318506172152417
Lambda = 0.01
MSE = 0.0018929919210726052
MAE = 0.026078589443550362
MPE = 0.0031837340219662224
Lambda = 0.02
MSE = 0.0018931011736174573
MAE = 0.026079502663176504
MPE = 0.003182406358664928
Lambda = 0.03
MSE = 0.0018932105036921824
MAE = 0.026080415857866527
MPE = 0.003181078731623752
Lambda = 0.04
MSE = 0.001893319911290241
MAE = 0.026081329027620698
MPE = 0.0031797511408368064
Lambda = 0.05
MSE = 0.001893429396405241
MAE = 0.026082242172440914
MPE = 0.0031784235863078123
Lambda = 0.06
MSE = 0.0018935389590306786
MAE = 0.02608315529232787
MPE = 0.003177096068033329
Lambda = 0.07
MSE = 0.0018936485991600436
MAE = 0.026084068387282132
MPE = 0.00317576858600923
Lambda = 0.08
MSE = 0.0018937583167869104
MAE = 0.02608498145730528
MPE = 0.0031744411402370907
Lambda = 0.09
MSE = 0.001893868111904762
MAE = 0.026085894502397775
MPE = 0.003173113730712418
Lambda = 0.1
MSE = 0.0018939779845071948
MAE = 0.026086807522561373
MPE = 0.003171786357437938

output of Kfold:

KFold(n_splits=5, random_state=None, shuffle=False)
Iteration Number = 0
MSE Error = 0.0024321326597404607
Iteration Number = 1
MSE Error = 0.0031141190254261133
Iteration Number = 2
MSE Error = 0.0023971068073798877
Iteration Number = 3
MSE Error = 0.001543071917380427
Iteration Number = 4
MSE Error = 0.002131483744103393
Final MSE Error on Test Data = 0.001972228198383395



3. If we observe the graph of L1,L2 regularisations we can see that as lambda increases error increases => bias increases and varience decreases => it avoids overfitting


